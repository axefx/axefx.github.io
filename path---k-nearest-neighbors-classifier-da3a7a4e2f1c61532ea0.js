webpackJsonp([0xb350a49cb49d],{"./node_modules/json-loader/index.js!./.cache/json/k-nearest-neighbors-classifier.json":function(e,i){e.exports={data:{markdownRemark:{html:'<h2 id="using-a-knn-classifier"><a href="#using-a-knn-classifier" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using A KNN Classifier</h2>\n<p>K-nearest neighbor classifier is considered a supervised machine learning algorithm. It is used to solve classification or regression challenges in a data analysis project. One example of a classification challenge is demonstrated in the “iris flower” dataset. The dataset contains data records of different iris plants and their physical measurements as well as the class or type of iris flower it belongs to. This dataset can be used to train a KNN algorithm that can classify the type of a new flower based on its physical properties.</p>\n<p><img src="https://www.pngkey.com/png/detail/82-826789_iris-iris-sepal-and-petal.png" alt="iris classes">\n<a href="https://scikit-learn.org/stable/datasets/index.html#iris-dataset">iris dataset</a></p>\n<h2 id="what-is-the-algorithm-doing"><a href="#what-is-the-algorithm-doing" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is the algorithm doing?</h2>\n<p>The K-nearest neighbors (KNN for short) algorithm expects a “K” parameter and its input training data to be vector representations of the data features being analyzed. The “K” refers to the number of neighbors to group by when training and storing the input data. The distance between the data inputs are measured, using different metrics such as <strong>Euclidean distance</strong>, and stores them into memory.<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/795b967db2917cdde7c2da2d1ee327eb673276c0" alt="eucledian distance">\nKNN algorithm can make use of a K dimensional tree (KD Tree) as its data structure. A KD tree is a “binary tree”, in which every leaf node is a k-dimensional point in a space matrix. The KD tree and the distance calculated can be used to store them as “neighbors” of each other. After training your algorithm to the dataset it can be used to find a classification of new data input. The KNN classifier uses a majority of votes method, by using the nearest neighbors returned by the data structure, to classify the new data point. The accuracy is impacted by the parameter “K” which groups which points should be considered neighbors, using an odd value for k helps avoid ties in votes.</p>\n<div style="width:60%;padding:5px5px;margin:auto;">\n<p><img src="https://raw.githubusercontent.com/axefx/axefx.github.io/source/content/sample-posts/06%E2%80%9325-2020-KNearest_Neighbors/kd-tree.png" alt="kd-tree"></p>\n</div>\n<h2 id="implementing-a-simple-knn-in-python"><a href="#implementing-a-simple-knn-in-python" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementing A Simple KNN in python</h2>\n<h3 id="dependencies"><a href="#dependencies" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dependencies:</h3>\n<ul>\n<li>KD tree data structure from scipy</li>\n<li>Counter from python collection</li>\n</ul>\n<h3 id="description"><a href="#description" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Description:</h3>\n<p>KNN algorithm as a python class<br>\nTakes k parameter in class instantiation</p>\n<ul>\n<li>\n<p>Fit method</p>\n<ul>\n<li>load X and y inputs into class variables</li>\n<li>pass X into the KD tree as class variable</li>\n</ul>\n</li>\n<li>\n<p>Predict method</p>\n<ul>\n<li>For each predict input query and save nearest_neighbors to class variable</li>\n<li>Create a list of labels from the nearest_neighbors</li>\n<li>Return the result of majority vote method on nearest_neighbors</li>\n</ul>\n</li>\n<li>\n<p>Majority vote method</p>\n<ul>\n<li>Use counter to count all labels of nearest_neighbors</li>\n<li>Use the most_common method</li>\n<li>repeat counting for all labels</li>\n</ul>\n</li>\n<li>\n<p>kneighbors method</p>\n<ul>\n<li>Does predict function and returns the nearest_neighbor class variable</li>\n</ul>\n</li>\n</ul>\n<h2 id="check-out-the-colab-notebook"><a href="#check-out-the-colab-notebook" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Check out the <a href="https://colab.research.google.com/drive/1fIKfky_xc0U4Rw5n2hIoVRS7sH9I0qwV?usp=sharing">colab notebook</a></h2>\n<h2 id="check-out-the-github-repo"><a href="#check-out-the-github-repo" aria-hidden="true" class="anchor"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Check out the <a href="https://github.com/axefx/CS-Data-Science-Build-Week-1/blob/master/src/knn.py">github repo</a></h2>',timeToRead:2,excerpt:"Using A KNN Classifier K-nearest neighbor classifier is considered a supervised machine learning algorithm. It is used to solve…",frontmatter:{title:"K-Nearest Neighbors Classifier",cover:"https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Tree_0001.svg/1024px-Tree_0001.svg.png",date:"06/25/20",category:"Data_Science",tags:["Data_Science"]},fields:{nextTitle:"K-Nearest Neighbors Classifier",nextSlug:"/k-nearest-neighbors-classifier",prevTitle:"K-Nearest Neighbors Classifier",prevSlug:"/k-nearest-neighbors-classifier",slug:"/k-nearest-neighbors-classifier"}}},pathContext:{slug:"/k-nearest-neighbors-classifier"}}}});
//# sourceMappingURL=path---k-nearest-neighbors-classifier-da3a7a4e2f1c61532ea0.js.map